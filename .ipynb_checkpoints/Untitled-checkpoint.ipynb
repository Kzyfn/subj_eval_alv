{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nnmnkwii.datasets import FileDataSource, FileSourceDataset\n",
    "from nnmnkwii.datasets import PaddedFileSourceDataset, MemoryCacheDataset  # これはなに？\n",
    "from nnmnkwii.preprocessing import trim_zeros_frames, remove_zeros_frames\n",
    "from nnmnkwii.preprocessing import minmax, meanvar, minmax_scale, scale\n",
    "from nnmnkwii import paramgen\n",
    "from nnmnkwii.io import hts\n",
    "from nnmnkwii.frontend import merlin as fe\n",
    "from nnmnkwii.postfilters import merlin_post_filter\n",
    "\n",
    "from os.path import join, expanduser, basename, splitext, basename, exists\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyworld\n",
    "import pysptk\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm\n",
    "import optuna\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "y_stats = pd.read_csv('../../asj_vae/data/y_stats.csv')\n",
    "\n",
    "\n",
    "from models import VQVAE\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mgc_dim = 180  # メルケプストラム次数　？？\n",
    "lf0_dim = 3  # 対数fo　？？ なんで次元が３？\n",
    "vuv_dim = 1  # 無声or 有声フラグ　？？\n",
    "bap_dim = 15  # 発話ごと非周期成分　？？\n",
    "\n",
    "duration_linguistic_dim = 438  # question_jp.hed で、ラベルに対する言語特徴量をルールベースで記述してる\n",
    "acoustic_linguisic_dim = 442  # 上のやつ+frame_features とは？？\n",
    "duration_dim = 1\n",
    "acoustic_dim = mgc_dim + lf0_dim + vuv_dim + bap_dim  # aoustice modelで求めたいもの\n",
    "\n",
    "fs = 48000\n",
    "frame_period = 5\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)\n",
    "hop_length = int(0.001 * frame_period * fs)\n",
    "\n",
    "mgc_start_idx = 0\n",
    "lf0_start_idx = 180\n",
    "vuv_start_idx = 183\n",
    "bap_start_idx = 184\n",
    "\n",
    "windows = [\n",
    "    (0, 0, np.array([1.0])),\n",
    "    (1, 1, np.array([-0.5, 0.0, 0.5])),\n",
    "    (1, 1, np.array([1.0, -2.0, 1.0])),\n",
    "]\n",
    "\n",
    "use_phone_alignment = True\n",
    "acoustic_subphone_features = \"coarse_coding\" if use_phone_alignment else \"full\"  # とは？\n",
    "\n",
    "import random, string\n",
    "\n",
    "\n",
    "def randomname(n):\n",
    "   randlst = [random.choice(string.ascii_letters + string.digits) for i in range(n)]\n",
    "   return ''.join(randlst)\n",
    "\n",
    "\n",
    "def gen_parameters(y_predicted, verbose=True):\n",
    "    # Number of time frames\n",
    "    T = y_predicted.shape[0]\n",
    "    \n",
    "    # Split acoustic features\n",
    "    mgc = y_predicted[:,:lf0_start_idx]\n",
    "    lf0 = y_predicted[:,lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = Y['acoustic']['train'][90][:, lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = np.zeros(lf0.shape)\n",
    "    vuv = y_predicted[:,vuv_start_idx]\n",
    "\n",
    "    plt.show()\n",
    "    bap = y_predicted[:,bap_start_idx:]\n",
    "    \n",
    "    # Perform MLPG\n",
    "    ty = \"acoustic\"\n",
    "    mgc_variances = np.tile(y_stats['var'][:lf0_start_idx], (T, 1))#np.tile(np.ones(Y_var[ty][:lf0_start_idx].shape), (T, 1))#\n",
    "    mgc = paramgen.mlpg(mgc, mgc_variances, windows)\n",
    "    lf0_variances = np.tile(y_stats['var'][lf0_start_idx:vuv_start_idx], (T,1))#np.tile(np.ones(Y_var[ty][lf0_start_idx:vuv_start_idx].shape), (T,1))#\n",
    "    lf0 = paramgen.mlpg(lf0, lf0_variances, windows)\n",
    "    bap_variances = np.tile(y_stats['var'][bap_start_idx:], (T, 1))#np.tile(np.ones(Y_var[ty][bap_start_idx:].shape), (T, 1))#\n",
    "    bap = paramgen.mlpg(bap, bap_variances, windows)\n",
    "    \n",
    "    return mgc, lf0, vuv, bap\n",
    "def gen_waveform(y_predicted, do_postfilter=True, verbose=True):  \n",
    "    y_predicted = trim_zeros_frames(y_predicted)\n",
    "        \n",
    "    # Generate parameters and split streams\n",
    "    mgc, lf0, vuv, bap = gen_parameters(y_predicted, verbose=verbose)\n",
    "    \n",
    "    if do_postfilter:\n",
    "        mgc = merlin_post_filter(mgc, alpha)\n",
    "        \n",
    "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
    "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), fs, fftlen)\n",
    "    f0 = lf0.copy()\n",
    "    f0[vuv < 0.5] = 0\n",
    "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
    "    \n",
    "    generated_waveform = pyworld.synthesize(f0.flatten().astype(np.float64),\n",
    "                                            spectrogram.astype(np.float64),\n",
    "                                            aperiodicity.astype(np.float64),\n",
    "                                            fs, frame_period)\n",
    "    return generated_waveform\n",
    "\n",
    "def class2value(cl, model):\n",
    "    codebook = np.sort(model.quantized_vectors.weight.detach().cpu().numpy().reshape(-1))#\n",
    "    return codebook[cl]\n",
    "\n",
    "def synthesize(z):\n",
    "\n",
    "\n",
    "    vqvae = VQVAE(num_layers=2, z_dim=1, num_class=4, input_linguistic_dim = 289+2).to(device)\n",
    "    vqvae.load_state_dict(torch.load('../../osaka_corpus/0929_tokyo3000included_alllabeled_lr1e-6/vqvae_model_40.pth', map_location=torch.device('cuda')))\n",
    "\n",
    "    DATA_ROOT = '../../asj_vae/data/basic5000'\n",
    "    data = [np.fromfile(join(DATA_ROOT, 'X_acoustic/BASIC5000_0001.bin')).reshape(-1, 442), np.fromfile(join(DATA_ROOT, 'Y_acoustic/BASIC5000_0001.bin')).reshape(-1, 199), np.loadtxt(join(DATA_ROOT, 'mora_index/squeezed_mora_index_0001.csv')).reshape(-1),]#水をマレーシアから買わなくてはならないのですのデータ\n",
    "\n",
    "    z_tf = np.array([class2value(int(cl), vqvae) for cl in z]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        linguistic_f = data[0]\n",
    "        linguistic_f = np.concatenate((linguistic_f[:, :285], linguistic_f[:, -4:], np.ones((linguistic_f.shape[0], 1)), np.zeros((linguistic_f.shape[0], 1))), axis=1)\n",
    "        linguistic_f = torch.from_numpy(linguistic_f).float().to(device)\n",
    "        pred_lf0 = vqvae.decode(torch.from_numpy(z_tf).float().to(device), linguistic_f, data[2], tokyo=False).cpu().numpy().reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "    y_base = data[1].copy()\n",
    "\n",
    "    y_base[:, lf0_start_idx] = pred_lf0\n",
    "    y_base[:, lf0_start_idx+1:lf0_start_idx+3] = 0\n",
    "\n",
    "    waveform = gen_waveform(y_base)\n",
    "\n",
    "    filepath = './static/wav/BASIC5000_0001_{}.wav'.format(randomname(10))\n",
    "\n",
    "    #wavfile.write(filepath, rate=fs, data=waveform.astype(np.int16))\n",
    "\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def recon_f0(data, vqvae_model):\n",
    "    tmp = [torch.from_numpy(np.concatenate([data[0], np.ones([data[0].shape[0], 1]), np.zeros([data[0].shape[0], 1])], axis=1)).float().to(device), \n",
    "          torch.from_numpy(data[1]).float().to(device)]\n",
    "    recon_y, z, z_uq = vqvae_model(tmp[0], tmp[1], data[2], 0)\n",
    "\n",
    "    return recon_y.detach().cpu().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,]\n",
    "\n",
    "vqvae = VQVAE(num_layers=2, z_dim=1, num_class=4, input_linguistic_dim = 289+2).to(device)\n",
    "vqvae.load_state_dict(torch.load('../../osaka_corpus/0929_tokyo3000included_alllabeled_lr1e-6/vqvae_model_40.pth', map_location=torch.device('cuda')))\n",
    "\n",
    "DATA_ROOT = '../../asj_vae/data/basic5000'\n",
    "data = [np.fromfile(join(DATA_ROOT, 'X_acoustic/BASIC5000_0001.bin'), dtype=np.float32).reshape(-1, 442), np.fromfile(join(DATA_ROOT, 'Y_acoustic/BASIC5000_0001.bin'), dtype=np.float32).reshape(-1, 199).astype('float64'), np.loadtxt(join(DATA_ROOT, 'mora_index/squeezed_mora_index_0001.csv')).reshape(-1),]#水をマレーシアから買わなくてはならないのですのデータ\n",
    "\n",
    "z_tf = np.array([class2value(int(cl), vqvae) for cl in z]).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kazuya_yufune/miniconda3/envs/vqvae/lib/python3.6/site-packages/pysptk/conversion.py:154: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(np.fft.rfft(symc).real)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    linguistic_f = data[0]\n",
    "    linguistic_f = np.concatenate((linguistic_f[:, :285], linguistic_f[:, -4:], np.ones((linguistic_f.shape[0], 1)), np.zeros((linguistic_f.shape[0], 1))), axis=1)\n",
    "    linguistic_f = torch.from_numpy(linguistic_f).float().to(device)\n",
    "    pred_lf0 = vqvae.decode(torch.from_numpy(z_tf).float().to(device), linguistic_f, data[2], tokyo=False).cpu().numpy().reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "y_base = data[1].copy()\n",
    "\n",
    "y_base[:, lf0_start_idx] = pred_lf0\n",
    "y_base[:, lf0_start_idx+1:lf0_start_idx+3] = 0\n",
    "\n",
    "waveform = gen_waveform(y_base)\n",
    "\n",
    "filepath = './static/wav/BASIC5000_0001_{}.wav'.format(randomname(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.71535641e-01,  2.15019704e-03,  1.20977885e-04, ...,\n",
       "         4.64962082e+01,  5.39838942e+01, -6.85134882e-09],\n",
       "       [ 1.23589649e+01,  9.09890419e-03,  4.27969323e-04, ...,\n",
       "         7.45628880e-07, -2.08068268e+02,  3.61511535e+02],\n",
       "       [ 1.66227913e+01,  1.48021746e-02,  9.18192816e-05, ...,\n",
       "        -7.70071777e+03, -3.31271410e+00, -2.81814919e+01],\n",
       "       ...,\n",
       "       [ 3.40292513e-01,  4.24932149e-05,  4.48375709e-11, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.55617452e+00,  1.34409924e-06,  1.29050619e-07, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 7.79918789e-01,  2.03873241e-05,  6.01745426e-10, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
